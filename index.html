<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Streaming Cache Test</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }

        .container {
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .explanation {
            background-color: #e3f2fd;
            border-left: 4px solid #1976d2;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 4px;
        }

        .explanation h2 {
            margin-top: 0;
            color: #1976d2;
        }

        .form-group {
            margin-bottom: 15px;
        }

        label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }

        input,
        textarea {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 4px;
            box-sizing: border-box;
        }

        .button-group {
            display: flex;
            gap: 10px;
            margin-top: 20px;
        }

        button {
            background-color: #007bff;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }

        button:hover {
            background-color: #0056b3;
        }

        button:disabled {
            background-color: #6c757d;
            cursor: not-allowed;
        }

        button.secondary {
            background-color: #6c757d;
        }

        button.secondary:hover {
            background-color: #5a6268;
        }

        .response-container {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin-top: 20px;
        }

        .response-box {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 4px;
            padding: 15px;
            height: 400px;
            overflow-y: auto;
        }

        .response-box h3 {
            margin-top: 0;
            color: #333;
        }

        .response-content {
            white-space: pre-wrap;
            word-wrap: break-word;
            font-family: 'Courier New', monospace;
            font-size: 14px;
        }

        .status {
            margin-bottom: 10px;
            font-weight: bold;
            padding: 5px;
            border-radius: 4px;
        }

        .status.streaming {
            background-color: #d1ecf1;
            color: #0c5460;
        }

        .status.complete {
            background-color: #d4edda;
            color: #155724;
        }

        .status.error {
            background-color: #f8d7da;
            color: #721c24;
        }

        .timing-info {
            font-size: 12px;
            color: #666;
            margin-top: 5px;
        }

        .loading-spinner {
            display: inline-block;
            width: 16px;
            height: 16px;
            border: 2px solid #f3f3f3;
            border-top: 2px solid #007bff;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            margin-left: 10px;
        }

        @keyframes spin {
            0% {
                transform: rotate(0deg);
            }

            100% {
                transform: rotate(360deg);
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>LLM Streaming Cache Test</h1>

        <div class="explanation">
            <h2>What does this worker do?</h2>
            <p>This Cloudflare Worker provides an intelligent caching layer for Large Language Model (LLM) API requests:
            </p>
            <ul>
                <li><strong>Request Deduplication:</strong> When multiple identical requests are made, the worker
                    ensures only one API call goes to the LLM provider</li>
                <li><strong>Live Streaming:</strong> The first request streams the response in real-time from the LLM
                </li>
                <li><strong>Smart Broadcasting:</strong> Subsequent identical requests receive the same stream without
                    making additional API calls</li>
                <li><strong>Response Caching:</strong> Completed responses are cached in Cloudflare KV for instant
                    retrieval</li>
                <li><strong>Cost Optimization:</strong> Reduces API costs by preventing duplicate LLM requests</li>
            </ul>
            <p>This test demonstrates the behavior with three simultaneous requests: immediate, 1-second delayed, and
                5-second delayed.</p>
        </div>

        <div class="form-group">
            <label for="endpoint">Endpoint URL:</label>
            <input type="text" id="endpoint" value="/https://api.openai.com/v1/chat/completions">
        </div>

        <div class="form-group">
            <label for="apiKey">OpenAI API Key:</label>
            <input type="password" id="apiKey" placeholder="sk-...">
        </div>

        <div class="form-group">
            <label for="prompt">Prompt:</label>
            <textarea id="prompt" rows="6"
                placeholder="Enter your prompt here...">Tell me a short story about a robot learning to paint.</textarea>
        </div>

        <div class="button-group">
            <button id="testButton" onclick="runTest()">Run Test</button>
            <button id="randomizeButton" class="secondary" onclick="randomizePrompt()">
                Randomize Prompt (Dad Jokes)
            </button>
            <button id="clearButton" class="secondary" onclick="clearResponses()">Clear Responses</button>
        </div>

        <div class="response-container">
            <div class="response-box">
                <h3>Request 1 (Immediate)</h3>
                <div id="status1" class="status">Ready</div>
                <div id="timing1" class="timing-info"></div>
                <div id="response1" class="response-content"></div>
            </div>

            <div class="response-box">
                <h3>Request 2 (1s delay)</h3>
                <div id="status2" class="status">Ready</div>
                <div id="timing2" class="timing-info"></div>
                <div id="response2" class="response-content"></div>
            </div>

            <div class="response-box">
                <h3>Request 3 (5s delay)</h3>
                <div id="status3" class="status">Ready</div>
                <div id="timing3" class="timing-info"></div>
                <div id="response3" class="response-content"></div>
            </div>
        </div>
    </div>

    <script>
        let decoder = new TextDecoder();

        // Load saved values from localStorage
        document.addEventListener('DOMContentLoaded', () => {
            const savedEndpoint = localStorage.getItem('llm-cache-endpoint');
            const savedApiKey = localStorage.getItem('llm-cache-apikey');

            if (savedEndpoint) {
                document.getElementById('endpoint').value = savedEndpoint;
            }
            if (savedApiKey) {
                document.getElementById('apiKey').value = savedApiKey;
            }
        });

        async function makeRequest(requestId, delay = 0) {
            const endpoint = document.getElementById('endpoint').value;
            const apiKey = document.getElementById('apiKey').value;
            const prompt = document.getElementById('prompt').value;

            const statusEl = document.getElementById(`status${requestId}`);
            const responseEl = document.getElementById(`response${requestId}`);
            const timingEl = document.getElementById(`timing${requestId}`);

            // Reset the response box
            responseEl.textContent = '';
            timingEl.textContent = '';

            if (delay > 0) {
                statusEl.textContent = `Waiting ${delay}ms...`;
                statusEl.className = 'status';
                await new Promise(resolve => setTimeout(resolve, delay));
            }

            const startTime = performance.now();
            statusEl.textContent = 'Streaming...';
            statusEl.className = 'status streaming';

            try {
                const response = await fetch(endpoint, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        model: 'gpt-3.5-turbo',
                        messages: [
                            { role: 'user', content: prompt }
                        ],
                        stream: true
                    })
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}- ${await response.text()}`);
                }

                const reader = response.body.getReader();
                let buffer = '';
                let firstChunkTime = null;

                while (true) {
                    const { done, value } = await reader.read();

                    if (done) break;

                    if (firstChunkTime === null) {
                        firstChunkTime = performance.now() - startTime;
                        timingEl.textContent = `First chunk: ${firstChunkTime.toFixed(0)}ms`;
                    }

                    // Decode the chunk
                    buffer += decoder.decode(value, { stream: true });

                    // Process complete SSE messages
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || ''; // Keep incomplete line in buffer

                    for (const line of lines) {
                        if (line.trim() === '') continue;
                        if (line.startsWith('data: ')) {
                            const data = line.slice(6);
                            if (data === '[DONE]') continue;

                            try {
                                const json = JSON.parse(data);
                                if (json.choices && json.choices[0].delta && json.choices[0].delta.content) {
                                    responseEl.textContent += json.choices[0].delta.content;
                                }
                            } catch (e) {
                                console.error('Error parsing JSON:', e);
                            }
                        }
                    }
                }

                const endTime = performance.now() - startTime;
                statusEl.textContent = 'Complete';
                statusEl.className = 'status complete';
                timingEl.textContent += ` | Total: ${endTime.toFixed(0)}ms`;

            } catch (error) {
                statusEl.textContent = `Error: ${error.message}`;
                statusEl.className = 'status error';
                responseEl.textContent = error.stack || error.message;
            }
        }

        async function runTest() {
            const testButton = document.getElementById('testButton');
            testButton.disabled = true;

            // Save values to localStorage
            const endpoint = document.getElementById('endpoint').value;
            const apiKey = document.getElementById('apiKey').value;
            localStorage.setItem('llm-cache-endpoint', endpoint);
            localStorage.setItem('llm-cache-apikey', apiKey);

            try {
                // Clear previous responses
                clearResponses();

                // Start all three requests with their respective delays
                await Promise.all([
                    makeRequest(1, 0),      // Immediate
                    makeRequest(2, 1000),   // 1 second delay
                    makeRequest(3, 5000)    // 5 second delay
                ]);

            } finally {
                testButton.disabled = false;
            }
        }

        async function randomizePrompt() {
            const button = document.getElementById('randomizeButton');
            const promptEl = document.getElementById('prompt');

            // Add loading spinner
            button.innerHTML = 'Loading... <span class="loading-spinner"></span>';
            button.disabled = true;

            try {
                // Fetch 5 random dad jokes
                const jokes = [];
                for (let i = 0; i < 5; i++) {
                    const response = await fetch('https://icanhazdadjoke.com/', {
                        headers: { 'Accept': 'application/json' }
                    });
                    const data = await response.json();
                    jokes.push(data.joke);
                }

                // Create the prompt
                const jokesList = jokes.map((joke, index) => `${index + 1}. ${joke}`).join('\n');
                const prompt = `Here are 5 dad jokes:\n\n${jokesList}\n\nBased on these examples, please create 20 more dad jokes in a similar style. Make them funny, punny, and family-friendly!`;

                promptEl.value = prompt;
            } catch (error) {
                console.error('Error fetching jokes:', error);
                alert('Failed to fetch dad jokes. Please try again.');
            } finally {
                button.innerHTML = 'Randomize Prompt (Dad Jokes)';
                button.disabled = false;
            }
        }

        function clearResponses() {
            for (let i = 1; i <= 3; i++) {
                document.getElementById(`status${i}`).textContent = 'Ready';
                document.getElementById(`status${i}`).className = 'status';
                document.getElementById(`response${i}`).textContent = '';
                document.getElementById(`timing${i}`).textContent = '';
            }
        }
    </script>
</body>

</html>